{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正規化層\n",
    "\n",
    "*Normalization Layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力されたデータを正規化して返す層。\n",
    "\n",
    "正規化する軸方向に依って色々な種類がある。\n",
    "\n",
    "- バッチ正規化\n",
    "- 層正規化\n",
    "- インスタンス正規化\n",
    "- グループ正規化\n",
    "\n",
    "pytorchで挙動を見ながら理解していこう。  \n",
    "- https://pytorch.org/docs/stable/nn.html#normalization-layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用語\n",
    "\n",
    "- サンプル: バッチ内の各データ\n",
    "- 特徴量: サンプルを構成する各スカラー\n",
    "- チャンネル: サンプルを構成するテンソルの1番上の次元。サンプル内の各データ。\n",
    "- インスタンス: チャンネル内の各データ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## バッチ正規化\n",
    "\n",
    "*Batch Normalization*\n",
    "\n",
    "[1] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" International conference on machine learning. pmlr, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ内を正規化し、学習したパラメータに従ってスケーリング・シフトして出力する層。バッチ内は特徴量ごとに正規化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチサイズ$m$のミニバッチ$\\mathcal B = \\{\\boldsymbol x^{(1)},\\boldsymbol x^{(2)},\\cdots,\\boldsymbol x^{(m)} \\}$が得られた時、以下の演算で入力$\\boldsymbol x^{(n)}$に対する出力値$\\boldsymbol y^{(n)}$を決定する。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "\\boldsymbol\\mu_{\\mathcal B} &= \\frac{1}{m}\\sum_{n=1}^m \\boldsymbol x^{(n)} \\\\\n",
    "\\boldsymbol\\sigma^2_{\\mathcal B} &= \\frac{1}{m}\\sum_{i=1}^m (\\boldsymbol x^{(n)} - \\boldsymbol\\mu_{\\mathcal B})^2 \\\\\n",
    "\n",
    "\\hat{\\boldsymbol x}^{(n)} &= \\frac{\\boldsymbol x^{(n)} - \\boldsymbol\\mu_{\\mathcal B}}{\\sqrt{\\boldsymbol\\sigma^2_{\\mathcal B} + \\epsilon}} \\\\\n",
    "\n",
    "\\boldsymbol y^{(n)} &= \\boldsymbol\\gamma\\hat{\\boldsymbol x}^{(n)} + \\boldsymbol\\beta\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "- $\\boldsymbol x^{(n)}, \\, \\hat{\\boldsymbol x}^{(n)}, \\, \\boldsymbol y^{(n)}, \\, \\boldsymbol\\mu_{\\mathcal B}, \\, \\boldsymbol\\sigma^2_{\\mathcal B}, \\, \\boldsymbol\\gamma, \\, \\boldsymbol\\beta \\in \\R^d$\n",
    "- $d$: 特徴量の数\n",
    "- $\\epsilon$: 微小値（0除算回避用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずミニバッチ$\\mathcal B$の平均$\\boldsymbol\\mu_{\\mathcal B}$と分散$\\boldsymbol\\sigma^2_{\\mathcal B}$を求める。次にそれらを用いて$\\boldsymbol x_i$を正規化する。最後に$\\boldsymbol\\gamma,\\boldsymbol\\beta$を用いてスケーリングとシフトを行う。$\\boldsymbol\\gamma,\\boldsymbol\\beta$は学習可能なパラメータで、出力データ$\\boldsymbol y^{(i)}$の分散と平均を意味する。まとめると、この層は、分布（分散$\\boldsymbol\\gamma$、平均$\\boldsymbol\\beta$）を学習し、その分布に従うように入力データを変換する層ということ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、上記の演算は学習時に行うもので、推論時には使えない。ミニバッチ内の他のデータに依って出力が変わってしまうため。推論時は、$\\boldsymbol y_i$は$\\boldsymbol x_i$のみに依存している必要がある。また推論時はバッチサイズが1であることも多く、その場合$\\hat{\\boldsymbol x}_i$が$\\boldsymbol 0$になるため$\\boldsymbol y_i$が$\\boldsymbol\\beta$に固定されてしまう。\n",
    "\n",
    "推論時は以下のような演算を行う。\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol x} = \\frac{\\boldsymbol x - \\mathbb E[\\boldsymbol x]}{\\sqrt{\\text{Var}[\\boldsymbol x] + \\epsilon}} \\\\\n",
    "$$\n",
    "\n",
    "入力データ$\\boldsymbol x$の平均$\\mathbb E[\\boldsymbol x]$と分散$\\text{Var}[\\boldsymbol x]$を用いる。これらは学習時に観測したデータから求めることになるため、実質的に学習データ全体の平均と分散となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNでのバッチ正規化\n",
    "\n",
    "画像や畳み込み層からの出力は3次元のデータで表される。当然これらも同じように正規化することが可能である。サンプルの形状が`(c, h, w)`の場合、$c\\times h\\times w$個ずつパラメータ（平均、分散）を用意し、特徴量ごとに正規化（&スケーリング・シフト）するということ。\n",
    "\n",
    "ただこの方法は基本的に使わず、実際はチャンネルごとに正規化する。サンプルを跨いだ同じ特徴マップの値を全て同じ種類の特徴量とみなし、それらを同時に正規化する。パラメータは平均と分散がチャンネルの数だけ必要になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かに、各チャンネル各ピクセルの値を獨立した別々の特徴量として扱うのは違和感があるので、チャンネルごとに正規化するのは自然に感じる。論文[1]には\n",
    "\n",
    "> For convolutional layers, we additionally want the normalization to obey the convolutional property (畳み込みレイヤーの場合、さらに正規化は畳み込みの性質に従うようにしたい。(Deepl訳))\n",
    "\n",
    "と書いてあった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装\n",
    "\n",
    "PyTorchで実装して挙動を確認してみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず学習時の挙動を確認する。適当なミニバッチを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "num_features = 4\n",
    "x = torch.arange(0, num_features * batch_size, dtype=torch.float32)\n",
    "x = x.reshape(batch_size, num_features)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータも初期化しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = torch.ones(num_features)\n",
    "beta = torch.zeros(num_features)\n",
    "eps = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均は全て0、分散は全て1で初期化した。\n",
    "\n",
    "では正規化層の演算を実装する。まずミニバッチの統計量を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均: tensor([4., 5., 6., 7.])\n",
      "分散: tensor([16., 16., 16., 16.])\n"
     ]
    }
   ],
   "source": [
    "mean = x.mean(dim=0)\n",
    "var = x.var(dim=0)\n",
    "print(\"平均:\", mean)\n",
    "print(\"分散:\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dim=0`でバッチ軸を指定し、特徴量ごとの平均と分散を求めた。なお、本当は`x.var(dim=0, unbiased=False)`として標本分散を求めないといけない。ただ不偏分散の方が分かりやすいため、ここではそうしちゃう。\n",
    "\n",
    "次は正規化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = (x - mean) / torch.sqrt(var + eps)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均0、分散1になった。\n",
    "\n",
    "最後にパラメータでスケーリングとシフトを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gamma * x_hat + beta\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今はパラメータが平均0、分散1なので変化なし。\n",
    "\n",
    "以上がバッチ正規化の演算である。これを`nn.Module`として実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=0)\n",
    "        var = x.var(dim=0)\n",
    "        x_hat = (x - mean) / (torch.sqrt(var) + self.eps)\n",
    "        y = x_hat * self.gamma + self.beta\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = BatchNormalization(num_features)\n",
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、簡易バッチ正規化層の完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、ちゃんとしたバッチ正規化層も作ってみよう。推論時の挙動を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training: # 学習\n",
    "            mean = x.mean(dim=0)\n",
    "            var = x.var(dim=0, unbiased=False)\n",
    "            var_unbiased = x.var(dim=0, unbiased=True)\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var_unbiased\n",
    "        else: # 推論\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        x_hat = (x - mean) / (torch.sqrt(var) + self.eps)\n",
    "        y = x_hat * self.gamma + self.beta\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb E[\\boldsymbol x]$、$\\text{Var}[\\boldsymbol x]$を`running_mean`、`running_var`として保持し、推論時に使う。そしてそれらは学習時に都度更新する。移動平均によって動的に求めている。$\\text{Var}[\\boldsymbol x]$は不偏分散なので`unbiased=True`にする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習モードで適当にデータを見せると`running_mean`、`running_var`が更新される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('beta', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([0., 0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1., 1.]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初期値\n",
    "norm = BatchNormalization(num_features)\n",
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('beta', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([ 0.0527, -0.0417, -0.0876, -0.0537])),\n",
       "             ('running_var', tensor([0.9879, 0.4089, 1.3890, 0.7146]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.train()\n",
    "torch.manual_seed(0)\n",
    "for _ in range(100):\n",
    "    x = torch.randn(batch_size, num_features)\n",
    "    norm(x)\n",
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論モードにすると`running_mean`、`running_var`が使われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8423, -1.5081,  1.0178,  2.3462]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.eval()\n",
    "x = torch.randn(1, num_features)\n",
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こういうこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8423, -1.5081,  1.0178,  2.3462]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma, beta, running_mean, running_var = norm.state_dict().values()\n",
    "eps = norm.eps\n",
    "y = (x - running_mean) / torch.sqrt(running_var + eps) * gamma + beta\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchに実装されている層\n",
    "\n",
    "先ほど実装した`BatchNormalization`は`nn.BatchNorm1d`としてそのままPyTorchに実装されている。\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挙動もほぼ同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([0., 0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1., 1.])),\n",
       "             ('num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = nn.BatchNorm1d(num_features)\n",
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([ 0.0527, -0.0417, -0.0876, -0.0537])),\n",
       "             ('running_var', tensor([0.9879, 0.4089, 1.3890, 0.7146])),\n",
       "             ('num_batches_tracked', tensor(100))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.train()\n",
    "torch.manual_seed(0)\n",
    "for _ in range(100):\n",
    "    x = torch.randn(batch_size, num_features)\n",
    "    norm(x)\n",
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8423, -1.5081,  1.0178,  2.3462]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.eval()\n",
    "x = torch.randn(1, num_features)\n",
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ値になったね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.BatchNorm1d`はチャンネル軸が加わった3次元のデータに対しても使用できる。チャンネルごとに正規化を行う。\n",
    "\n",
    "<br>\n",
    "\n",
    "ここでのチャンネル軸というのは、サンプルを表した2階以上のテンソルの1番上の軸のこと。CNNで画像を扱うときによく見る。画像以外で使われている場面はあまり見ないが、とりあえず[公式ドキュメント](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)でそう呼ばれていたので倣った。\n",
    "\n",
    "チャンネル軸が加わった3次元のデータと表現したが、時系列データという解釈もできて、実際に公式ドキュメントでは3つ目の軸を`sequence length`と呼んでいる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "c = 3\n",
    "num_features = 4\n",
    "x = torch.arange(batch_size * c * num_features, dtype=torch.float32)\n",
    "x = x.reshape(batch_size, c, num_features)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373]],\n",
       "\n",
       "        [[ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288]]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = nn.BatchNorm1d(c)\n",
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こういうこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 7.5000],\n",
       "          [11.5000],\n",
       "          [15.5000]]]),\n",
       " tensor([[[37.2500],\n",
       "          [37.2500],\n",
       "          [37.2500]]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = x.mean(dim=(0, 2), keepdim=True)\n",
    "var = x.var(dim=(0, 2), unbiased=False, keepdim=True)\n",
    "gamma = norm.weight.reshape(1, c, 1)\n",
    "beta = norm.bias.reshape(1, c, 1)\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373]],\n",
       "\n",
       "        [[ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x - mean) / torch.sqrt(var + eps) * gamma + beta\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各チャンネルが2次元の場合は`nn.BatchNorm2d`を使う。また3次元の場合は`nn.BatchNorm3d`を使う。4次元以上は多分ない。\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html\n",
    "\n",
    "`nn.BatchNorm2d`はCNNでよく使う。先で説明したCNNの場合の動作と同じ動きをする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "c, w, h = 3, 224, 224\n",
    "x = torch.randn(batch_size, c, w, h)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0.])),\n",
       "             ('running_mean', tensor([0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1.])),\n",
       "             ('num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = nn.BatchNorm2d(c)\n",
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータの数はチャンネルの数と一緒。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = norm(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 層正規化\n",
    "\n",
    "*Layer Normalization*\n",
    "\n",
    "[2] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer normalization.\" arXiv preprint arXiv:1607.06450 (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルごとに正規化し、特徴量ごとにスケーリング・シフトして出力する層。バッチ正規化から正規化の方法が変わっただけ（スケーリング・シフトは一緒）。RNNにバッチ正規化が適用しづらいということで提案された。\n",
    "\n",
    "層正規化はバッチ正規化同様、特徴量の数だけパラメータを持つ。また、この層は演算結果がバッチ内の他のデータに依らないため、学習時と推論時で挙動が変わらない（変える必要がない）。そのため、推論用の統計量を保持する必要がない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\mu^{(n)} &= \\frac{1}{d}\\sum_{i=1}^d x_i^{(n)} \\\\\n",
    "{\\sigma^2}^{(n)} &= \\frac{1}{d}\\sum_{i=1}^d (x_i^{(n)} - \\mu^{(n)})^2 \\\\\n",
    "\\hat{\\boldsymbol x}^{(n)} &= \\frac{\\boldsymbol x^{(n)} - \\mu^{(n)}}{\\sqrt{{\\sigma^2}^{(n)} + \\epsilon}} \\\\\n",
    "\\boldsymbol y^{(n)} &= \\hat{\\boldsymbol x}^{(n)}\\boldsymbol\\gamma + \\boldsymbol\\beta \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol x^{(n)}, \\, \\hat{\\boldsymbol x}^{(n)}, \\, \\boldsymbol y^{(n)}, \\, \\boldsymbol\\gamma, \\, \\boldsymbol\\beta \\in \\R^d$\n",
    "- $\\mu^{(n)}, \\, {\\sigma^2}^{(n)} \\in \\R$\n",
    "- $\\boldsymbol x^{(n)} = (x_1^{(n)}, x_2^{(n)}, \\cdots, x_d^{(n)})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装\n",
    "\n",
    "PyTorchで実装して挙動を確認してみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適当なミニバッチを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "d = 4\n",
    "x = torch.arange(batch_size * d, dtype=torch.float32).reshape(batch_size, d)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータの初期化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = torch.ones(d)\n",
    "beta = torch.zeros(d)\n",
    "eps = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルごとの統計量を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.5000],\n",
       "         [ 5.5000],\n",
       "         [ 9.5000],\n",
       "         [13.5000]]),\n",
       " tensor([[1.2500],\n",
       "         [1.2500],\n",
       "         [1.2500],\n",
       "         [1.2500]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = x.mean(dim=1, keepdim=True)\n",
    "var = x.var(dim=1, unbiased=False, keepdim=True)\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dim=1`とした（バッチ正規化では`0`）。`keepdim=True`は後のコードを複雑にしないためのもの。\n",
    "\n",
    "次に正規化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = (x - mean) / torch.sqrt(var + eps)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルごとに正規化された。\n",
    "\n",
    "最後にスケーリングとシフトを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gamma * x_hat + beta\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今はパラメータが平均0、分散1なので変化なし。\n",
    "\n",
    "以上が層正規化の演算である。ちなみに、バッチ正規化だとこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -1.3416, -1.3416, -1.3416],\n",
       "        [-0.4472, -0.4472, -0.4472, -0.4472],\n",
       "        [ 0.4472,  0.4472,  0.4472,  0.4472],\n",
       "        [ 1.3416,  1.3416,  1.3416,  1.3416]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = nn.BatchNorm1d(d)\n",
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "軸が違うのが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、これを`nn.Module`として実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, unbiased=False, keepdim=True)\n",
    "        x_hat = (x - mean) / (torch.sqrt(var) + self.eps)\n",
    "        y = x_hat * self.gamma + self.beta\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = LayerNorm(d)\n",
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ結果が得られた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchにも実装されている。\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = nn.LayerNorm(d)\n",
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらも同じ結果が得られた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量は多次元でも可能。また特徴量以外の軸を好きに足してもいい。初めに与えた形状と合致する部分を内側の軸から探してくれる。\n",
    "\n",
    "例えばRNNではこうなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d = 4\n",
    "x = torch.arange(batch_size * seq_len * d, dtype=torch.float32)\n",
    "x = x.reshape(batch_size, seq_len, d)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
       "\n",
       "        [[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = nn.LayerNorm(d)\n",
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各時刻各サンプルが正規化された。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次元の特徴量も可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "c, w, h = 3, 224, 224\n",
    "x = torch.randn(batch_size, c, w, h)\n",
    "\n",
    "norm = nn.LayerNorm((c, w, h))\n",
    "y = norm(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えた形状と同じ形状のパラメータが用意される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([3, 224, 224]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.weight.shape, norm.bias.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
