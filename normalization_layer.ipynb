{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正規化層\n",
    "\n",
    "*Normalization Layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力されたデータを正規化して返す層。\n",
    "\n",
    "正規化する軸方向に依って色々な種類がある。\n",
    "\n",
    "- バッチ正規化\n",
    "- 層正規化\n",
    "- インスタンス正規化\n",
    "- グループ正規化\n",
    "\n",
    "pytorchで挙動を見ながら理解していこう。  \n",
    "- https://pytorch.org/docs/stable/nn.html#normalization-layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用語\n",
    "\n",
    "- サンプル: バッチ内の各データ\n",
    "- 特徴量: サンプルを構成する各スカラー\n",
    "- チャンネル: サンプルを構成するテンソルの1番上の次元。サンプル内の各データ。\n",
    "- インスタンス: チャンネル内の各データ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## バッチ正規化\n",
    "\n",
    "*Batch Normalization*\n",
    "\n",
    "[1] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" International conference on machine learning. pmlr, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ内を正規化し、学習したパラメータに従ってスケーリング・シフトして出力する層。バッチ内は特徴量ごとに正規化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチサイズ$m$のミニバッチ$\\mathcal B = \\{\\boldsymbol x_1,\\boldsymbol x_2,\\cdots,\\boldsymbol x_m \\}$が得られた時、以下の演算で出力値$\\boldsymbol y_i$を決定する。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "\\boldsymbol\\mu_{\\mathcal B} &= \\frac{1}{m}\\sum_{i=1}^m \\boldsymbol x_i \\\\\n",
    "\\boldsymbol\\sigma^2_{\\mathcal B} &= \\frac{1}{m}\\sum_{i=1}^m (\\boldsymbol x_i - \\boldsymbol\\mu_{\\mathcal B})^2 \\\\\n",
    "\n",
    "\\hat{\\boldsymbol x}_i &= \\frac{\\boldsymbol x_i - \\boldsymbol\\mu_{\\mathcal B}}{\\sqrt{\\boldsymbol\\sigma^2_{\\mathcal B} + \\epsilon}} \\\\\n",
    "\n",
    "\\boldsymbol y_i &= \\boldsymbol\\gamma\\hat{\\boldsymbol x}_i + \\boldsymbol\\beta\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "- $\\boldsymbol x_i, \\, \\hat{\\boldsymbol x}_i, \\, \\boldsymbol y_i, \\, \\boldsymbol\\mu_{\\mathcal B}, \\, \\boldsymbol\\sigma^2_{\\mathcal B}, \\, \\boldsymbol\\gamma, \\, \\boldsymbol\\beta \\in \\R^d$\n",
    "- $d$: 特徴量の数\n",
    "- $\\epsilon$: 微小値（0除算回避用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずミニバッチ$\\mathcal B$の平均$\\boldsymbol\\mu_{\\mathcal B}$と分散$\\boldsymbol\\sigma^2_{\\mathcal B}$を求める。次にそれらを用いて$\\boldsymbol x_i$を正規化する。最後に$\\boldsymbol\\gamma,\\boldsymbol\\beta$を用いてスケーリングとシフトを行う。$\\boldsymbol\\gamma,\\boldsymbol\\beta$は学習可能なパラメータで、出力データ$\\boldsymbol y_i$の分散と平均を意味する。まとめると、この層は、分布（分散$\\boldsymbol\\gamma$、平均$\\boldsymbol\\beta$）を学習し、その分布に従うように入力データを変換する層ということ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、上記の演算は学習時に行うもので、推論時には使えない。ミニバッチ内の他のデータに依って出力が変わってしまうため。推論時は、$\\boldsymbol y_i$は$\\boldsymbol x_i$のみに依存している必要がある。また推論時はバッチサイズが1であることも多く、その場合$\\hat{\\boldsymbol x}_i$が$\\boldsymbol 0$になるため$\\boldsymbol y_i$が$\\boldsymbol\\beta$に固定されてしまう。\n",
    "\n",
    "推論時は以下のような演算を行う。\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol x} = \\frac{\\boldsymbol x - \\mathbb E[\\boldsymbol x]}{\\sqrt{\\text{Var}[\\boldsymbol x] + \\epsilon}} \\\\\n",
    "$$\n",
    "\n",
    "入力データ$\\boldsymbol x$の平均$\\mathbb E[\\boldsymbol x]$と分散$\\text{Var}[\\boldsymbol x]$を用いる。これらは学習時に観測したデータから求めることになるため、実質的に学習データ全体の平均と分散となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNでのバッチ正規化\n",
    "\n",
    "画像や畳み込み層からの出力は3次元のデータで表される。当然これらも同じように正規化することが可能である。サンプルの形状が`(c, h, w)`の場合、$c\\times h\\times w$個ずつパラメータ（平均、分散）を用意し、特徴量ごとに正規化（&スケーリング・シフト）するということ。\n",
    "\n",
    "ただこの方法は基本的に使わず、実際はチャンネルごとに正規化する。サンプルを跨いだ同じ特徴マップの値を全て同じ種類の特徴量とみなし、それらを同時に正規化する。パラメータは平均と分散がチャンネルの数だけ必要になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かに、各チャンネル各ピクセルの値を獨立した別々の特徴量として扱うのは違和感があるので、チャンネルごとに正規化するのは自然に感じる。論文[1]には\n",
    "\n",
    "> For convolutional layers, we additionally want the normalization to obey the convolutional property (畳み込みレイヤーの場合、さらに正規化は畳み込みの性質に従うようにしたい。(Deepl訳))\n",
    "\n",
    "と書いてあった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装\n",
    "\n",
    "PyTorchで実装して挙動を確認してみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず学習時の挙動を確認する。適当なミニバッチを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = 4\n",
    "batch_size = 3\n",
    "x = torch.arange(0, num_features * batch_size, dtype=torch.float32)\n",
    "x = x.reshape(batch_size, num_features)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータも初期化しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = torch.ones(num_features)\n",
    "beta = torch.zeros(num_features)\n",
    "eps = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均は全て0、分散は全て1で初期化した。\n",
    "\n",
    "では正規化層の演算を実装する。まずミニバッチの統計量を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均: tensor([4., 5., 6., 7.])\n",
      "分散: tensor([16., 16., 16., 16.])\n"
     ]
    }
   ],
   "source": [
    "mean = x.mean(dim=0)\n",
    "var = x.var(dim=0)\n",
    "print(\"平均:\", mean)\n",
    "print(\"分散:\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dim=0`でバッチ軸を指定し、特徴量ごとの平均と分散を求めた。なお、本当は`x.var(dim=0, unbiased=False)`として標本分散を求めないといけない。ただ不偏分散の方が分かりやすいため、ここではそうしちゃう。\n",
    "\n",
    "次は正規化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = (x - mean) / torch.sqrt(var + eps)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均0、分散1になった。\n",
    "\n",
    "最後にパラメータでスケーリングとシフトを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gamma * x_hat + beta\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今はパラメータが平均0、分散1なので変化なし。\n",
    "\n",
    "以上がバッチ正規化の演算である。これを`nn.Module`として実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=0)\n",
    "        var = x.var(dim=0)\n",
    "        x_hat = (x - mean) / (torch.sqrt(var) + self.eps)\n",
    "        y = x_hat * self.gamma + self.beta\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = BatchNormalization(num_features)\n",
    "y = bn(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、簡易バッチ正規化層の完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、ちゃんとしたバッチ正規化層も作ってみよう。推論時の挙動を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training: # 学習\n",
    "            mean = x.mean(dim=0)\n",
    "            var = x.var(dim=0, unbiased=False)\n",
    "            var_unbiased = x.var(dim=0, unbiased=True)\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var_unbiased\n",
    "        else: # 推論\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        x_hat = (x - mean) / (torch.sqrt(var) + self.eps)\n",
    "        y = x_hat * self.gamma + self.beta\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb E[\\boldsymbol x]$、$\\text{Var}[\\boldsymbol x]$を`running_mean`、`running_var`として保持し、推論時に使う。そしてそれらは学習時に都度更新する。移動平均によって動的に求めている。$\\text{Var}[\\boldsymbol x]$は不偏分散なので`unbiased=True`にする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習モードで適当にデータを見せると`running_mean`、`running_var`が更新される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('beta', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([0., 0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1., 1.]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初期値\n",
    "bn = BatchNormalization(num_features)\n",
    "bn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('beta', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([ 0.0527, -0.0417, -0.0876, -0.0537])),\n",
       "             ('running_var', tensor([0.9879, 0.4089, 1.3890, 0.7146]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.train()\n",
    "torch.manual_seed(0)\n",
    "for _ in range(100):\n",
    "    x = torch.randn(batch_size, num_features)\n",
    "    bn(x)\n",
    "bn.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論モードにすると`running_mean`、`running_var`が使われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8423, -1.5081,  1.0178,  2.3462]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.eval()\n",
    "x = torch.randn(1, num_features)\n",
    "y = bn(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こういうこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8423, -1.5081,  1.0178,  2.3462]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma, beta, running_mean, running_var = bn.state_dict().values()\n",
    "eps = bn.eps\n",
    "y = (x - running_mean) / torch.sqrt(running_var + eps) * gamma + beta\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchに実装されている層\n",
    "\n",
    "先ほど実装した`BatchNormalization`は`nn.BatchNorm1d`としてそのままPyTorchに実装されている。\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挙動もほぼ同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([0., 0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1., 1.])),\n",
       "             ('num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(num_features)\n",
    "bn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0., 0.])),\n",
       "             ('running_mean', tensor([ 0.0527, -0.0417, -0.0876, -0.0537])),\n",
       "             ('running_var', tensor([0.9879, 0.4089, 1.3890, 0.7146])),\n",
       "             ('num_batches_tracked', tensor(100))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.train()\n",
    "torch.manual_seed(0)\n",
    "for _ in range(100):\n",
    "    x = torch.randn(batch_size, num_features)\n",
    "    bn(x)\n",
    "bn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8423, -1.5081,  1.0178,  2.3462]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.eval()\n",
    "x = torch.randn(1, num_features)\n",
    "y = bn(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ値になったね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.BatchNorm1d`はチャンネル軸が加わった3次元のデータに対しても使用できる。チャンネルごとに正規化を行う。\n",
    "\n",
    "<br>\n",
    "\n",
    "ここでのチャンネル軸というのは、サンプルを表した2階以上のテンソルの1番上の軸のこと。CNNで画像を扱うときによく見る。画像以外で使われている場面はあまり見ないが、とりあえず[公式ドキュメント](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)でそう呼ばれていたので倣った。\n",
    "\n",
    "チャンネル軸が加わった3次元のデータと表現したが、時系列データという解釈もできて、実際に公式ドキュメントでは3つ目の軸を`sequence length`と呼んでいる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "c = 3\n",
    "num_features = 4\n",
    "x = torch.arange(0, batch_size * c * num_features, dtype=torch.float32)\n",
    "x = x.reshape(batch_size, c, num_features)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373]],\n",
       "\n",
       "        [[ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288]]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(c)\n",
    "y = bn(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こういうこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 7.5000],\n",
       "          [11.5000],\n",
       "          [15.5000]]]),\n",
       " tensor([[[37.2500],\n",
       "          [37.2500],\n",
       "          [37.2500]]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = x.mean(dim=(0, 2), keepdim=True)\n",
    "var = x.var(dim=(0, 2), unbiased=False, keepdim=True)\n",
    "gamma = bn.weight.reshape(1, c, 1)\n",
    "beta = bn.bias.reshape(1, c, 1)\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373],\n",
       "         [-1.2288, -1.0650, -0.9012, -0.7373]],\n",
       "\n",
       "        [[ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288],\n",
       "         [ 0.7373,  0.9012,  1.0650,  1.2288]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x - mean) / torch.sqrt(var + eps) * gamma + beta\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各チャンネルが2次元の場合は`nn.BatchNorm2d`を使う。また3次元の場合は`nn.BatchNorm3d`を使う。4次元以上は多分ない。\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html\n",
    "\n",
    "`nn.BatchNorm2d`はCNNでよく使う。先で説明したCNNの場合の動作と同じ動きをする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "c, w, h = 3, 224, 224\n",
    "x = torch.randn(batch_size, c, w, h)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0.])),\n",
       "             ('running_mean', tensor([0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1.])),\n",
       "             ('num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm2d(c)\n",
    "bn.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータの数はチャンネルの数と一緒。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 層正規化\n",
    "\n",
    "*Layer Normalization*\n",
    "\n",
    "Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer normalization.\" arXiv preprint arXiv:1607.06450 (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルごとに正規化し、特徴量ごとにスケーリング・シフトして出力する層。特徴量の数だけパラメータを持つ。また、この層は演算結果がバッチ内の他のデータに依らないため、学習時と推論時で挙動が変わらない（変える必要がない）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "d = 5\n",
    "x = torch.arange(batch_size * d).reshape(batch_size, d).to(torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層作成。  \n",
    "特徴量の形状を与える。その形状と同じ平均と分散がパラメータとなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "norm = nn.LayerNorm(d)\n",
    "for params in norm.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データ全体の統計量は持っていない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4142, -0.7071,  0.0000,  0.7071,  1.4142],\n",
       "        [-1.4142, -0.7071,  0.0000,  0.7071,  1.4142]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下と同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4142, -0.7071,  0.0000,  0.7071,  1.4142],\n",
       "        [-1.4142, -0.7071,  0.0000,  0.7071,  1.4142]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "((x - mean) / torch.sqrt(var + norm.eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "統計量を求める軸を層（特徴量）方向に指定した（`dim=-1`）。\n",
    "\n",
    "特徴量は何次元でもいい。例えばRNNの場合、以下のようにすれば各バッチ各隠れ状態が正規化される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d = 4\n",
    "x = torch.arange(batch_size * seq_len * d).reshape(batch_size, seq_len, d).to(torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "norm = nn.LayerNorm(d)\n",
    "for params in norm.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
       "\n",
       "        [[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = norm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
       "\n",
       "        [[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "         [-1.3416, -0.4472,  0.4472,  1.3416]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "((x - mean) / torch.sqrt(var + norm.eps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
