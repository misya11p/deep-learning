---
---

# ニューラルネットワーク

---

# 1. ニューラルネットワークとは

人間の脳の神経回路を模した数理モデル。もう少しいうと、機械学習モデル。

- 数理モデル

ある現象を数式で記述したもの  
→ ここでは**関数**と捉えておくと良い。なんらかの数字が入力されると、なんらかの数字を出力するもの。

- 機械学習モデル

入力に対して適切な出力が行えるように**学習**できるモデル

図

---

## 機械学習モデル

前提知識として、機械学習モデルについてもう少し詳しく説明する。

機械学習モデルは、入力に対して適切な出力が行うように学習することが可能なモデルである。このモデル学習させることで得られる優れたモデルは、人間が行うようなタスクをこなすことが出来る。

例

| タスク | 入力 | 出力 |
| --- | --- | --- |
| 画像分類 | 猫の画像 | 猫 |
| 将棋 | 盤面 | 次の一手 |
| 会話 | 質問 | 回答 |

---

機械学習モデルの入出力は数値である必要がある。機械学習モデルを適応させたいタスクでは、数値として表せる入出力を考えることが重要。

先程の例で考えると、例えば画像は各画素の色を表す数値の集合で表せる。また画像分類の結果は、分類する種類の数だけ整数を割り当てれば良い。犬と猫の分類なら、どちらかに0、どちらかに1を割り当てれば良い。

ここでの数値は厳密にはテンソルである。

---

### 機械学習

機械学習モデルを学習させることを機械学習という。

機械学習モデルは関数であった。機械学習の目的は、**いい関数**を得ることと言える。

ここで、「関数内の数式」は**事前に決まっている**。例えば以下の様な式。

$$
f(x) = ax + b
$$

しかし、式の中の具体的な数値は決まっていない。上の式でいうと、$a$と$b$を指す。「入力に何かをかけて何かを足す」といった処理の内容は決まっているが、具体的に何をかけて何を足すかは決まっていないということ。この「何か（$a$, $b$）」のことをパラメータといい、最も適切なパラメータを求める事が機械学習の目的である。

---

## ニューラルネットワーク

以上を踏まえた上で、改めてニューラルネットワークについて説明する。

初めに述べた通り、ニューラルネットワークは人間の脳の神経回路を模した機械学習モデルである。

ニューラルネットワークは他の機械学習モデルに比べて表現力に長けており、多様な入力に対して適切な出力を行うことができる。また拡張性も高く、様々な多様な使い方が考案されている。近年話題のAIには全てニューラルネットワークが使われていると言っても過言ではない。

この資料ではそんなニューラルネットワークについて学んでいく。

---

# 2. ニューラルネットワークの構造

NN（ニューラルネットワーク）のイメージはこんな感じ

図

では、これらを構成する要素についてみていこう。

---

## パーセプトロン

NNを構成する要素。図の〇がパーセプトロン。  
パーセプトロンも1つのモデルとみることが出来て、入力に対してパラメータに基づいた出力を行う。

パーセプトロンは複数の入力値を受け取り、0か1を出力する。  
パラメータは二種類あり、重みとバイアスである。入力値に重みをかけたものの和（線形和）とバイアスを足したものが0以上なら1、0未満なら0を出力する。入力を$x$、出力を$y$、重みを$w$、バイアスを$b$とすると、以下の式で表せる。

$$
y = \left\{
\begin{array}{ll}
1 & (x_1 w_1 + x_2 w_2 + \cdots + x_n w_n \geq b) \\
0 & (x_1 w_1 + x_2 w_2 + \cdots + x_n w_n  < b)
\end{array}
\right.
$$

まとめると以下。
入力をベクトル$x$、重みをベクトル$w$で表す。あと今後分かりやすくするために$b$を移行する。

$$
y = \left\{
\begin{array}{ll}
1 & (x \cdot w - b \geq 0) \\
0 & (x \cdot w - b < 0)
\end{array}
\right.
$$

$x = (x_1, x_2, \cdots , x_n)$  
$w = (w_1, w_2, \cdots , w_n)$


---

Pythonで実装してみよう。入力する値は二つで、パラメータは適当。

```python
def perceptron(x1, x2):
    x = [x1, x2] # 入力をベクトル(1次元配列)に変換
    w = [0.5, 0.5] # 重み
    b = 0.7 # バイアス

    if np.dot(x, w) - b >= 0: # 内積がバイアス以上のとき
        return 1 # 1を出力
    else:
        return 0
```

適当に値を入れてみると

```python
y = perceptron(2, -1)
print(y)
>>> 0
```

$(2 \times 0.5) + (-1 \times 0.5) = 0.5 < 0.7$ と、内積(0.5)がバイアス(0.7)を超えなかったので0が出力された

---

### 人工ニューロン

人間の脳の神経細胞（ニューロン）を模した数理モデル。
パーセプトロンの出力を実数全体に拡張したもの。内積にバイアスを足した値を**活性化関数**と呼ばれる関数に入れたときの値を出力する。

活性化関数を$h$とすると、以下の式で表せる。

$$
y = h(x \cdot w + b)
$$

実装してみる

```python
activation = lambda x: 2*x # 活性化関数

def neuron(x1, x2):
    x = [x1, x2]
    w = [0.5, 0.5]
    b = 0.7

    y = activation(np.dot(x, w) + b)
    return y
```

適当に値を入れてみると

```python
y = neuron(2, -3)
print(y)
>>> 0.3999999999999999
```

$((2 \times 0.5) + (-3 \times 0.5) + 0.7) \times 2 = 0.4$と、（誤差が生じているが）正しい値が出力された。

---

## ニューラルネットワーク

人工ニューロンを何個も組み合わせたモデル。先程の図を再掲。

図

各人工ニューロンが別々のパラメータを持っている。NNの学習は、それらをすべて最適化することとなる。

---

### 演算の流れ

それぞれの人工ニューロンは、前の人工ニューロンからの出力を入力にとり、演算結果(出力)を次の人工ニューロンに渡す。これを繰り返して最終的に出力された値がそのニューラルネットワークの出力となる。

---

### 層

NNは層をひとつの単位として扱う。層はNNの縦一列を指す。

最初の層は入力層、最後の層は出力層、それ以外の層は中間層または隠れ層と呼ぶ。

図

2つ以上の中間層を持つものはディープニューラルネットワークとも呼ばれる。このディープニューラルネットワークの学習は**ディープラーニング**と呼ばれ、これを和訳したものが**深層学習**である。

---

活性化関数は各人工ニューロンに対して設定できるが、NNを考える場合は層ごとに設定する。というか活性化関数を一つの層として考えたほうがよい。学習可能なパラメータを持たず、ただ数値を変換するだけの層。そして人工ニューロンは内積とバイアスの和をそのまま出力するだけ。

図

---

### 全結合層

人工ニューロンを縦に並べて構成した層。詳細は後程述べるが、NNを構築する層には様々な種類がある。その中で、この全結合層はNNを構築する上で最も基本的な層。

$$
y = x \cdot W + b
$$

```python
class Linear:
    def __init__(self, input_size, output_size):
        self.W = np.random.randn(input_size, output_size)
        self.b = np.random.randn(output_size)

    def forward(self, x):
        return np.dot(x, self.W) + self.b
```

全結合層には他にも様々な呼び方がある。

- 線形層
- linear
- fc (fully connected)
- dense
- affine

---

## 活性化関数

層と層の間に挟む関数。非線形な関数を採用することで、NNの表現力を高める。（全結合層の場合）これがないとNNが多層である意味がない=1層のNNと出来ることが変わらない。

---

### ReLU

最も一般的な活性化関数。入力が0以下の場合は0を出力し、0より大きい場合は入力をそのまま出力する。

$$
y = \left\{
\begin{array}{ll}
x & (x \geq 0) \\
0 & (x < 0)
\end{array}
\right.
$$

```python
class ReLU:
    def forward(self, x):
        return x * (x > 0)
```

---

### Leaky ReLU

ReLUの改良版。入力が0以下場合にも微小な傾きを設定する。

$$
y = \left\{
\begin{array}{ll}
x & (x \geq 0) \\
ax & (x < 0)
\end{array}
\right.
$$

```python
class LeakyReLU:
    def __init__(self, a):
        self.a = a

    def forward(self, x):
        return np.where(x > 0, x, self.a * x)
```

---

### Sigmoid

0から1の値を出力する。0~1の値を出力したい場合（出力値に最大値と最小値を定めたい場合。Ex: 確率, 色）に用いる。

$$
y = \frac{1}{1 + e^{-x}}
$$

```python
class Sigmoid:
    def forward(self, x):
        return 1 / (1 + np.exp(-x))
```

---

### tanh

sigmoidの範囲を-1から1に拡大したもの。sigmoidよりこっちを使った方が上手くいくといった場面がある。

$$
y = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

```python
class Tanh:
    def forward(self, x):
        return np.exp(x) - np.exp(-x) / np.exp(x) + np.exp(-x)
```

---

### Softmax

分類を行うNNの出力層に用いる。各クラスに対する確率を出力する。

$$
y_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
$$

```python
class Softmax:
    def forward(self, x):
        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)
```

---

## 色々な層

全結合層以外にも様々な層が存在する。

---

### Dropout

過学習を防ぐために用いられる層。入力された値を確率で0にして出力する。この層は学習時にのみ使用する。

$$
y = \left\{
\begin{array}{ll}
x & (確率p) \\
0 & (確率1-p)
\end{array}
\right.
$$

学習可能なパラメータは持たない

```python
class Dropout:
    def __init__(self, p):
        self.p = p

    def forward(self, x):
        return x * np.random.binomial(1, self.p, size=x.shape)
```

---

### Batch Normalization

バッチ正規化。Dropout同様、過学習防止に役立つ。また学習を安定させる面でも有効。

$$
y = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

```python
class BatchNormalization:
    def __init__(self, gamma, beta):
        self.gamma = gamma
        self.beta = beta

    def forward(self, x):
        mu = np.mean(x, axis=0)
        sigma = np.std(x, axis=0)
        return self.gamma * (x - mu) / (sigma + 1e-7) + self.beta
```

---

### 畳み込み層

画像のような2次元のデータを扱う際に用いることのできる層。カーネルと呼ばれる行列を用意し、入力データのある範囲との内積の様なものを全ての範囲で取る。

```python
class Conv2d():
    pass
```

![gif](https://kenyu-life.com/wp-content/uploads/2019/03/cnn.gif)

---

### 再帰層

時系列データを扱う際に用いることのできる層。前の時刻の出力を次の時刻の入力に取り入れる。

```python
class RNN():
    pass
```

---

## 層と活性化関数の違い

活性化関数は入力テンソルの1つ1つの値に着目し、全ての値・試行で同じ計算を行う。そのため、入出力でテンソルの形状が変わらない。また学習可能なパラメータを持たないことも特徴の一つ。

図

一方で層は入力テンソル全体を見て計算を行う。そのため、入出力でテンソルの形状が変わる場合がある。また、試行ごとに結果が変わる場合もある。

図

---

# 3. 学習

NNの学習を最適化問題に落とし込むことで学習を行う

- 最適化問題：ある関数を最大または最小にする変数を求める問題。この関数は目的関数と呼ぶ。

目的関数と変数を以下のように設定する。
- 目的関数：NNの出力と正解の差
- 変数：NNが持つパラメータ

こうすることで、NNの学習が「出力と正解との差を最小にするパラメータを求める」という最適化問題として捉えられる。

また出力と正解の差は**損失**または**誤差**と呼ばれる。

---

## 損失関数

NNの出力と正解の差を表す関数。NNの学習で目的関数として設定される場合が多い。

---

### 平均二乗誤差（*MSE - Mean Squared Error*）

$$
E = \frac{1}{n} \sum_{i} (y_i - t_i)^2
$$

---

### 交差エントロピー（*Cross Entropy*）

$$
E = - \frac{1}{n} \sum_{i} t_i \log y_i
$$

---

## 勾配降下法

最適化問題を解くアルゴリズムの一つで、NNの学習を行う最も基本的な方法。

以下を繰り返すことで目的関数を最適化する。
1. 変数の初期値を乱数などで設定する
2. 目的関数を変数で微分し、勾配を求める
3. 勾配に合わせて変数を更新する

終わらせるタイミングは色々
- 一定回数繰り返す
- 目的関数の値が閾値を満たす
- 変数が収束する（変化が閾値以下になる）

---

例

以下の関数を勾配降下法で最小化する。

$f(x) = x^2$

変数は$x$で、勾配は$f'(x) = 2x$となる。

勾配が正の時は、変数の値を減らすことで目的関数の値が小さくなる。一方で勾配が負の時は、変数の値を増やすことで目的関数の値が小さくなる。  
つまり、$x$の更新は以下の式に基づく。

$$
x := x - /eta f'(x)
$$

$\eta$は学習率といい、更新する度合いを調整するためのもの。基本0~1の値をとる。

---

Pythonで実装してみよう

```python
import random
eta = 0.001 # 学習率
n = 100 # 学習回数

def f(x): # 目的関数
    return x ** 2

def df(x): # 目的関数の微分
    return 2 * x

x = random.uniform(-10, 10) # xの初期値を-10~10でランダムに設定
print(f"学習前: x = {x}, f(x) = {f(x)}")

for i in range(n):
    dx = df(x) # 勾配を求める
    x -= eta * dx # xの更新

print(f"学習後: x = {x}, f(x) = {f(x)}")
```

動画

---

また多くの深層学習ライブラリでは勾配の計算が自動で行われる。PyTorchを使って実装するとこうなる。

```python
import torch
eta = 0.001 # 学習率
n = 100 # 学習回数

def f(x): # 目的関数
    return x ** 2

x = torch.rand(requires_grad=True) * 20 - 10 # xの初期値を-10~10でランダムに設定
optim = torch.optim.SGD(x, lr=0.001) # 勾配降下法を扱うためのもの

print(f"学習前: x = {x.item()}, f(x) = {f(x).item()}")

for i in range(n):
    y = f(x) # 目的関数の値を計算
    y.backward() # 勾配を計算
    optim.step() # 勾配降下法に基づいて変数を更新
    optim.zero_grad() # 保存されている勾配をリセット

print(f"学習後: x = {x.item()}, f(x) = {f(x).item()}")
```

このコードの目的関数をNNにして、変数をNNのパラメータにすればNNの学習ができる。

---

## 誤差逆伝播法

勾配降下法で、NNのパラメータの勾配を求めるアルゴリズム

これとは別に記事を書いているのでそちらも参照
- [誤差逆伝播法をPythonで実装しながら理解する - Qiita](https://qiita.com/miya_ppp/items/fd916da9da5578185bc8)

---

# 4. 実装

実装してみよう

---
